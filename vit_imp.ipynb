{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3edf506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912ccf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b919ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_setup\n",
    "import engine\n",
    "from helper_functions import download_data, set_seeds, plot_loss_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee30dc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0459c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download dataset if none already exists\n",
    "#Dataset is pizza_steak_sushi\n",
    "image_path = download_data(\n",
    "    source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\", \n",
    "    destination=\"pizza_steak_sushi\"\n",
    ")\n",
    "image_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff67e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set-up test and train paths \n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcd7f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we set our image size and transform our images before puting them through \n",
    "#the dataloaders\n",
    "IMG_SIZE = 224\n",
    "\n",
    "#Create transform pipeline\n",
    "manual_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "print(f'Image transformer created: {manual_transforms}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d03d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vit states they used batch size of 4096 and until i switch to running on my PC where I \n",
    "# have a GPU that can handle that I will be using a batch size of 24\n",
    "BATCH_SIZE = 24 if device == \"cpu\" else 4096\n",
    "\n",
    "#Create data loaders\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=manual_transforms, #use previously defined transforms\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875eb2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets see if the dataloaders worked correctly by loading an image \n",
    "# first grab a batch of images from the train set \n",
    "image_batch, label_batch = next(iter(train_dataloader))\n",
    "\n",
    "# get a single image from the batch \n",
    "image, label = image_batch[0], label_batch[0]\n",
    "\n",
    "#view  the objects\n",
    "image.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12758af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#that didn't really visualize it but we can see the size and the the associaate label tensor \n",
    "# so lets plot it w/matplotlib\n",
    "plt.imshow(image.permute(1, 2, 0)) #rearrange image dimensions to suit matplotlib [color_channels, height, width] -> [height, width, color_channels]\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6925c63",
   "metadata": {},
   "source": [
    "mmmm good looking pizza, or steak, or sushi\n",
    "\n",
    "## now we are ready to actually replecate the paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a768a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by calculating the patch embedding input and output shapes\n",
    "# our training resolution is 224 x 224 (H x W) \n",
    "height = 224\n",
    "width = 224 \n",
    "color_channels = 3 # C \n",
    "patch_size = 16 # P - taken from column ViT-B/16 from table 5 in the ViT paper \n",
    "\n",
    "# calculate N (number of patches) \n",
    "number_of_patches = int((height * width) / patch_size**2)\n",
    "print(f\"Number of patches N w/ image height (H = {height}), width (W = {width}) and patch size (P = {patch_size}) is (N = {number_of_patches})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3edb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOw lets replicate the input and output shapes of the patch embedding layer \n",
    "# Input: Image starts as 2D w/size (H x W x C)\n",
    "embedding_layer_input_shape = (height, width, color_channels)\n",
    "\n",
    "# Output: Image gets converted to a sequence of flattened 2D patches w/size (N x (P^2 dot C))\n",
    "embedding_layer_output_shape = (number_of_patches, patch_size**2 * color_channels)\n",
    "print(f\"Input Shape (single 2D image): {embedding_layer_input_shape}\")\n",
    "print(f\"Output Shape (single 2d Image flattened into patches): {embedding_layer_output_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e141d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets convert an image into patches\n",
    "#change Image shape to be compatible w/matplotlib (color_channels, height, width) -> (height, width, color_channels)\n",
    "image_permuted = image.permute(1,2,0)\n",
    "\n",
    "#index to plot the top row of patched pixels\n",
    "patch_size = 16\n",
    "plt.figure(figsize=(patch_size, patch_size))\n",
    "plt.imshow(image_permuted[:patch_size, :, :])\n",
    "#this should show the top row of patched pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0bf46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets see a single patch \n",
    "plt.figure(figsize=(patch_size, patch_size))\n",
    "plt.imshow(image_permuted[:patch_size, :patch_size, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396400d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can turn this into individual patches \n",
    "#Setup hyperparameters and make sure image size and patch size are compatible \n",
    "img_size = 224\n",
    "# patch_size = 16 #Don't redefine instead reuse\n",
    "num_patches = img_size / patch_size \n",
    "assert img_size%patch_size == 0, \"Image size must be divisible by patch size\"\n",
    "print(f\"Number of patches per row: {num_patches} \\n Patch size: {patch_size} pixels x {patch_size} pixels\")\n",
    "\n",
    "#Create series of subplots \n",
    "fig, axs = plt.subplots(\n",
    "    nrows=1,\n",
    "    ncols=img_size//patch_size, # One column per patch\n",
    "    figsize=(num_patches, num_patches),\n",
    "    sharex=True,\n",
    "    sharey=True\n",
    ")\n",
    "\n",
    "#Iterate through number of patches in the top row \n",
    "for i, patch in enumerate(range(0, img_size, patch_size)):\n",
    "    axs[i].imshow(image_permuted[:patch_size, patch:patch+patch_size, :]) # Keep height index constant, alter width index\n",
    "    axs[i].set_xlabel(i+1) #Set label for patch number\n",
    "    axs[i].set_xticks([])\n",
    "    axs[i].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b84b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now how do we expand this for the full image? While keeping the correct hyperparameters, img_size and patch_size\n",
    "#img_size = 224\n",
    "#patch_size = 16 \n",
    "#num_patches = img_size/patch_size\n",
    "assert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\n",
    "print(f\"Number of patches per row: {num_patches}\\\n",
    "        \\nNumber of patches per column: {num_patches}\\\n",
    "        \\nTotal patches: {num_patches*num_patches}\\\n",
    "        \\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n",
    "    \n",
    "# again create series of subplots\n",
    "fig, axs = plt.subplots(\n",
    "    nrows=img_size//patch_size, # Need as int not float\n",
    "    ncols=img_size//patch_size,\n",
    "    figsize=(num_patches, num_patches),\n",
    "    sharex=True,\n",
    "    sharey=True\n",
    ")\n",
    "\n",
    "#loop through height and width this time\n",
    "for i, patch_height in enumerate(range(0, img_size, patch_size)): #iterate through height\n",
    "    for j, patch_width in enumerate(range(0, img_size, patch_size)): #iterate through width \n",
    "        #Plot permuted image patch \n",
    "        axs[i,j].imshow(image_permuted[\n",
    "            patch_height:patch_height+patch_size, # iterate height\n",
    "            patch_width:patch_width+patch_size, #iterate width\n",
    "            :]) # Get all color_channel\n",
    "        axs[i,j].set_ylabel(i+1, rotation='horizontal', horizontalalignment='right', verticalalignment='center')\n",
    "        axs[i,j].set_xlabel(j+1)\n",
    "        axs[i,j].set_xticks([])\n",
    "        axs[i,j].set_yticks([])\n",
    "        axs[i,j].label_outer()\n",
    "\n",
    "#Set super title for overall plot \n",
    "fig.suptitle(f\"{class_names[label]} -> Patchified Bitches\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20671861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doing that manually for all of our images would take forever \n",
    "# Instead we will use the torch Conv2d() to turn our image into patches of a convolutional neural network (CNN) feature maps\n",
    "# then use torch Flatten() for flattening the spatial dimensions of the feature map\n",
    "# patch_size = 16 \n",
    "\n",
    "# Create the Conv2d layer w/hyperparameters from the ViT paper \n",
    "conv2d = nn.Conv2d(\n",
    "    in_channels=3, # num color channels \n",
    "    out_channels=768, # From Table 1: Hidden size D, the embedding size \n",
    "    kernel_size=patch_size, # Can also use (patch_size, patch_size) \n",
    "    stride=patch_size, \n",
    "    padding=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ec923f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have our CNN layer lets see what we get when we pass an image through it\n",
    "plt.imshow(image.permute(1,2,0)) #adjust for matplotlib\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107d1ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass image through convolutional layer\n",
    "image_out_of_conv = conv2d(image.unsqueeze(0)) # add a single batch dimension (height, width, color_channels)\n",
    "print(image_out_of_conv.shape)\n",
    "\"\"\"Should show a torch vector / series of 768 \n",
    "can be read as \n",
    "torch.Size([1, 768, 14, 14]) -> [batch_size, embedding_dim, feature_map_height, feature_map_width]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f74f9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets visuallize some of these feature maps to see what they look like \n",
    "import random \n",
    "random_indexes = random.sample(range(0, 768), k=5) # pick 5 random number between 0 and embedding_size \n",
    "print(f\"Showing random convolutional feature maps from indexes: {random_indexes}\")\n",
    "\n",
    "#create plot \n",
    "fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(12,12))\n",
    "\n",
    "#Plot the randomly picked feature maps \n",
    "for i, idx in enumerate(random_indexes):\n",
    "    image_conv_feature_map = image_out_of_conv[:, idx, :, :] #index on the output tensor of the convolutional layer \n",
    "    axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy())\n",
    "    axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec0206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these don't look like much, almost like shrunken permuted versions of the entire image maybe\n",
    "# they should be representations of the major features of the image \n",
    "# these features may change over time as the neural network learns \n",
    "# What do they look like numerically? \n",
    "single_feature_map = image_out_of_conv[:, 0, :, :]\n",
    "single_feature_map, single_feature_map.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9becfa95",
   "metadata": {},
   "source": [
    "the grad_fn output of the single_feature_map and the requires_grad=True attribute \n",
    "\n",
    "\n",
    "means that PyTorch is tracking the gradients of this feature map and it will be updated \n",
    "\n",
    "\n",
    "by gradient descent during training \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35341f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#We've turned our image into patch embeddings but htey're still in the 2D format\n",
    "# Now we need to flatten them, PyTorch should have a handy tool to do that \n",
    "#whats our current shape again?\n",
    "print(f\"Current tenshor shape: {image_out_of_conv.shape} -> [batch, embedding_dim, feature_map_height, feature_map_width]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa9a038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've got the 768 part (P^2 dot C) \n",
    "# Now we need the number of patches (N)\n",
    "\"\"\" To do this we need to flatten the tensor\n",
    "but don't want to flatten the whole tensor we only want to flatten the \n",
    "\"spatial dimensions of the feature map\"\n",
    "in our case it's teh feature_map_height and feature_map_width \n",
    "so lets create a torch.nn.Flatten() layer to flatten only those dimensions\n",
    "we can use the start_dim and end_dim parameteres to set that up \n",
    "\"\"\"\n",
    "flatten = nn.Flatten(\n",
    "    start_dim=2, # flatten feature_map_height (dim 2)\n",
    "    end_dim=3 # flatten feature_map_width (dim 3) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2f5f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets put it all together \n",
    "\"\"\"\n",
    "Step 1: take a single image \n",
    "Step 2: Put it through the convolutional layer (conv2d) to turn the image into a 2D feature map\n",
    "(or patch embeddings)\n",
    "Step 3: Flatten the 2D feature map into a single sequence \n",
    "\"\"\"\n",
    "# Step 1 take our single image and view it \n",
    "plt.imshow(image.permute(1,2,0)) #adjusted for matplotlib\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False)\n",
    "print(f\"Original image shape: {image.shape}\")\n",
    "\n",
    "# Step 1 turn it into feature maps \n",
    "image_out_of_conv = conv2d(image.unsqueeze(0)) #add batch dimension to avoid shape errors \n",
    "print(f\"Image feature map shape: {image_out_of_conv.shape}\")\n",
    "\n",
    "# Step 2 Flatten the feature maps \n",
    "image_out_of_conv_flattened = flatten(image_out_of_conv)\n",
    "print(f\"Flattened image feature map shape: {image_out_of_conv_flattened.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1950297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are so close to teh desired shape outlined in ViT paper \n",
    "# Desired output (Flattened 2D patches): (196, 768) (N * (P^2 dot C))\n",
    "# Current shape: (1, 768,  196)\n",
    "# Only idfference is current shape has batch size and the dimensions are in reversed order to\n",
    "# the desired output \n",
    "\n",
    "#Lets rearrange the dimensions using Tensor.permute()\n",
    "image_out_of_conv_flattened_reshaped = image_out_of_conv_flattened.permute(0,2,1) #[batch_size, P^2•C, N] -> [batch_size, N, P^2•C]\n",
    "print(f\"Patch embedding sequence shape: {image_out_of_conv_flattened_reshaped.shape} -> [batch_size, num_patches, embedding_size]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267ed36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sweet now we've matched the desired input & output shapes for the patch embedding layer of the ViT architecture \n",
    "#using just 2 PyTorch layers\n",
    "# Lets visualize again \n",
    "\n",
    "#Get a single flattned feature map \n",
    "single_flattened_feature_map = image_out_of_conv_flattened_reshaped[:,:,0] # Index: (batch_size, num patches, embedding_dimension)\n",
    "\n",
    "# Plot the flattened feature map\n",
    "plt.figure(figsize=(22,22))\n",
    "plt.imshow(single_flattened_feature_map.detach().numpy())\n",
    "plt.title(f\"Flattened feature map shape: {single_flattened_feature_map.shape}\")\n",
    "plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99e041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks weird \n",
    "# May be due to the original transformer architecture was made to work with text \n",
    "# Vision Transformer architecture (ViT)'s goal was to use the original Transformer for images\n",
    "# What does it look like in tensor form?\n",
    "single_flattened_feature_map, single_flattened_feature_map.requires_grad, single_flattened_feature_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0794b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets combine these steps into a single PyTorch layer \n",
    "# We'll do it by sublassing nn.Module and creating a small PyTorch \"model\" \n",
    "# to do all the steps above \n",
    "\"\"\"\n",
    "Specifically we'll:\n",
    "1: Create a class called PatchEmbedding which subclasses nn.Module \n",
    "2: Initialize the class w/ parameters in_channels=3, patch_size=16 and embedding_dim=768 \n",
    "3: Create a layer to turn an image into patches using nn.Conv2d \n",
    "4: Create a layer to flatten the patch feature maps into a single dimension\n",
    "5: Define a forward() method to take an input  and pass it through the layers created in 3 & 4\n",
    "6: Make sure the outputshape reflects the required output shape of the ViT architecutre \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd89c6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: create a class\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Turns 2D input image into a 1D sequence learnable embedding vector\n",
    "    \n",
    "    Args: \n",
    "    in_channels (int): Number of color channels for the input images. Defaults to 3 \n",
    "    patch_size (int): Size of patches to convert input image into. Defaults to 16\n",
    "    embedding_dim (int): Size of embedding to turn image into. Defaults to 768 \n",
    "    \"\"\"\n",
    "    #Step 2: initialize class w/appropriate variables \n",
    "    def __init__(self, in_channels: int=3, patch_size: int=16, embedding_dim: int=768):\n",
    "        super().__init__()\n",
    "        self.in_channels=in_channels\n",
    "        self.patch_size=patch_size\n",
    "        self.embedding_dim=embedding_dim \n",
    "\n",
    "        #Step 3: Create a layer to turn image into patches \n",
    "        self.patcher = nn.Conv2d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.embedding_dim,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "            padding=0\n",
    "        )\n",
    "\n",
    "        #Step 4: Create a layer to flatten patch feature maps into single dimension \n",
    "        self.flatten = nn.Flatten(start_dim=2, end_dim=3) #Only flatten the feature map dimensions into a single vector\n",
    "    \n",
    "    #Step 5: Define forward method\n",
    "    def forward(self, x):\n",
    "        #Create assertion to check that inputs are correct shape \n",
    "        image_resolution = x.shape[-1]\n",
    "        assert image_resolution % self.patch_size == 0, f\"Input image size must be divisible by patch size \\n image shape: {image_resolution}, patch_size: {self.patch_size}\"\n",
    "\n",
    "        # Perform forward pass \n",
    "        x_patched = self.patcher(x)\n",
    "        x_flattened = self.flatten(x_patched) \n",
    "\n",
    "        # Step 6: make sure output shape has the right order \n",
    "        return x_flattened.permute(0,2,1) # adjusting so the embedding is the final dimension [batch_size, P^2•C, N] -> [batch_size, N, P^2•C]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612023a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets test it out \n",
    "set_seeds() \n",
    "\n",
    "# Create an instance of patch embedding layer \n",
    "patchify = PatchEmbedding(in_channels=3, patch_size=patch_size, embedding_dim=768)\n",
    "\n",
    "# Pass an image through \n",
    "print(f\"Input image shape: {image.unsqueeze(0).shape}\")\n",
    "patch_embedded_image = patchify(image.unsqueeze(0)) # Add an extra batch dimension on the 0th index, otherwise errors will occur\n",
    "print(f\"Ouptpu patch embedding shape: {patch_embedded_image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2721cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kewl \n",
    "# We've now replicated the patch embedding for eq 1 but not the class token/position embedding \n",
    "# we'll get there eventually have patience and keep working at it\n",
    "# all good things in their time \n",
    "\n",
    "# first lets get a summary of our PatchEmbedding layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b573962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random input sizex \n",
    "rando_input_image = (1,3,224,224)\n",
    "rando_input_image_error = (1,3,250,250) # will error because image size not compatible w/patch_size\n",
    "summary(\n",
    "    PatchEmbedding(),\n",
    "    input_size=rando_input_image, # Next try with rando_input_image_error\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14751449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with error \n",
    "summary(\n",
    "    PatchEmbedding(),\n",
    "    input_size=rando_input_image_error, # Next try with rando_input_image_error\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77824705",
   "metadata": {},
   "source": [
    "## Creating the class token embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35031dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we 've made the image patch embedding time to get to work on the class token embedding\n",
    "# View the patch embedding and patch embedding shape\n",
    "print(patch_embedded_image)\n",
    "print(f\"Patch embedding shape: {patch_embedded_image.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71e577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"To prepend a learnable embedding to the sequence of embedded patches we need\n",
    "to create a learnable embedding in the shape of the embedding_dimension (D) then add \n",
    "it to the number_of_patches dimension\n",
    "in pseudo-code \n",
    "patch_embedding = [image_patch_1, image_patch_2, image_patch_3...]\n",
    "class_token = learnable_embedding\n",
    "patch_embedding_with_class_token = torch.cat((class_token, patch_embedding), dim=1)\n",
    "\"\"\"\n",
    "#Get the batch size and embedding dimension \n",
    "batch_size = patch_embedded_image.shape[0]\n",
    "embedding_dimension = patch_embedded_image.shape[-1]\n",
    "\n",
    "#Create the class token embedding as a learnable parameter that shares the same size as \n",
    "# the embedding dimension (D) \n",
    "class_token = nn.Parameter(\n",
    "    torch.ones(batch_size, 1, embedding_dimension), # [batch_size, num_tokens, embedding_dimension]\n",
    "    requires_grad=True\n",
    ")\n",
    "\n",
    "# Show the first 10 examples of the class token \n",
    "print(class_token[:,:,:10])\n",
    "\n",
    "# Pring the class_token shape \n",
    "print(f\"Class token shape: {class_token.shape} -> [batch_size, num_tokens, embedding_dimension]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a7121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the class token embedding to the front of the patch embedding \n",
    "patch_embedded_image_w_class_embedding = torch.cat((class_token, patch_embedded_image), dim=1) #concat on first dim\n",
    "\n",
    "#Print the sequence of patch embeddings with the prepend class token embedding \n",
    "print(patch_embedded_image_w_class_embedding)\n",
    "print(f\"Sequence of patch embeddings with class token prepended shape: {patch_embedded_image_w_class_embedding.shape} -> [batch_size, num_patches, embedding_dimension]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ee0bca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
